{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=img/MScAI_brand.png width=70%></center>\n",
    "\n",
    "# Scikit-Learn: Feature Selection & Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Feature selection: we choose a subset of existing features\n",
    "* Feature engineering: we construct new features from existing data\n",
    "\n",
    "In Scikit-Learn, both are implemented as *transformers*: a `transform(X)` method, usually preceded by `fit(X)`. And optionally `fit_transform(X)` as a shortcut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature selection\n",
    "\n",
    "* **Filter approach**: calculate a statistic per feature and choose those above a threshold.\n",
    "* **Wrapper approach**: try different subsets and see which gives best performance after training.\n",
    "\n",
    "Reference:\n",
    "* https://scikit-learn.org/stable/modules/feature_selection.html#feature-selection\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Filter approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`VarianceThreshold` throws away features with too little variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "X = np.array([[0.5, 1.0, 0.1], \n",
    "              [0.3, 1.0, 0.2], \n",
    "              [0.1, 1.0, 0.2], \n",
    "              [0.9, 1.0, 0.2], \n",
    "              [0.8, 1.0, 0.1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "By default, only zero variance is thrown away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5, 0.1],\n",
       "       [0.3, 0.2],\n",
       "       [0.1, 0.2],\n",
       "       [0.9, 0.2],\n",
       "       [0.8, 0.1]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel = VarianceThreshold() \n",
    "sel.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But we can set the threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5],\n",
       "       [0.3],\n",
       "       [0.1],\n",
       "       [0.9],\n",
       "       [0.8]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel = VarianceThreshold(threshold=0.01) \n",
    "sel.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`SelectKBest` still considers just one feature at a time, but is more general. Eg we can use it to select features based on their relationship to the target $y$.\n",
    "\n",
    "The *chi-squared* ($\\chi^2$) score is a measure of correlation between a numerical feature and a discrete target, so it's suitable for this. We choose how many features to keep, eg if `k=2` here we will keep $X_2$ and $X_3$.\n",
    "\n",
    "<center><img src=img/feature_selection.svg width=55%></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores [ 10.81782088   3.7107283  116.31261309  67.0483602 ]\n",
      "Shapes (150, 4) (150, 2)\n"
     ]
    }
   ],
   "source": [
    "sel = SelectKBest(chi2, k=2).fit(X, y)\n",
    "X_new = sel.transform(X)\n",
    "print(\"Scores\", sel.scores_) # the chi-squared scores\n",
    "print(\"Shapes\", X.shape, X_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Wrapper approach\n",
    "\n",
    "The *wrapper* approach to feature selection is to try different subsets and see which gives best performance when used inside the ML model we want to use them in. There are at least three types of wrapper approach:\n",
    "\n",
    "* Forward\n",
    "* Backward\n",
    "* Metaheuristic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Backward feature elimination** works this way:\n",
    "\n",
    "1. Start with all features\n",
    "2. Train the model\n",
    "3. Inspect the feature importances, eg by looking at `coef_`, and choose the **least important**\n",
    "4. Remove that feature and repeat from 2.\n",
    "5. At the end, choose the best model of all those we've seen.\n",
    "\n",
    "* Implementation: [`RFECV`](https://scikit-learn.org/stable/modules/feature_selection.html#rfe)\n",
    "* [Example](https://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_with_cross_validation.html#sphx-glr-auto-examples-feature-selection-plot-rfe-with-cross-validation-py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature engineering\n",
    "\n",
    "* Scaling\n",
    "* Missing values\n",
    "* One-hot encoding\n",
    "* Arithmetic feature transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Scaling\n",
    "\n",
    "Some ML methods work better if features are normalised to $[0, 1]$ or standardised to have mean 0 and standard deviation 1. Scikit-Learn provides `StandardScaler`, for example, for the latter. The calculation is simple: $(X - \\bar{X}) / \\sigma(X)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=img/leak.jpg width=20%>\n",
    "    \n",
    "<font size=1>  \n",
    "Photo by <a href=\"https://unsplash.com/@sanatoga?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Joe Zlomek</a> on <a href=\"https://unsplash.com/photos/zrb_TkHPVtE?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash</a>\n",
    "</font></center>\n",
    "    \n",
    "We must not **leak information about the test set** into our training, so:\n",
    "\n",
    "* First, calculate the mean and std of the train set\n",
    "* Use them to transform the train set\n",
    "* Then use the same mean and std to transform the test set\n",
    "* Never use the mean and standard deviation of the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train = np.array([[0, 4, 1, 6, 7, 8, 5, 9.0]]).T\n",
    "X_test = np.array([[3.3, 4.5, 5.5]]).T\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "# do not fit on X_test!\n",
    "X_test = scaler.transform(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As a result, `X_train` is now standardised:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.66666667],\n",
       "       [-0.33333333],\n",
       "       [-1.33333333],\n",
       "       [ 0.33333333],\n",
       "       [ 0.66666667],\n",
       "       [ 1.        ],\n",
       "       [ 0.        ],\n",
       "       [ 1.33333333]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note `X_test` will not have a zero mean and unit variance. This is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.56666667],\n",
       "       [-0.16666667],\n",
       "       [ 0.16666667]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Imputing missing values\n",
    "\n",
    "It's common to have missing values in our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.],\n",
       "       [ 4.],\n",
       "       [ 1.],\n",
       "       [ 6.],\n",
       "       [ 7.],\n",
       "       [nan],\n",
       "       [ 5.],\n",
       "       [ 9.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[0, 4, 1, 6, 7, np.nan, 5, 9.0]]).T\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A common strategy is just to impute the mean of the values present in the column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [4.        ],\n",
       "       [1.        ],\n",
       "       [6.        ],\n",
       "       [7.        ],\n",
       "       [4.57142857],\n",
       "       [5.        ],\n",
       "       [9.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(strategy='mean')\n",
    "X2 = imp.fit_transform(X)\n",
    "X2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But perhaps a safer approach is to drop any rows containing missing values. In Pandas you can use `dropna`, while in Numpy you can use code like this. Again, we won't over-write our `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = X[~np.isnan(X).any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How does this work? `isnan` returns a 2D `array` of `bool`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Which we reduce to 1D using `any`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False,  True, False, False])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(X).any(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But be careful! Suppose we also have `y` and we're planning to train a model with `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([0, 1, 1, 0, 0, 1, 1, 0])\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " Then we must drop the **same rows** in `y`, based on the missing values in `X`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = y[~np.isnan(X).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 1) (7,)\n"
     ]
    }
   ],
   "source": [
    "print(X2.shape, y2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Arithmetic feature transformations\n",
    "\n",
    "<center><img src=img/arithmetic-feature-transformation.png width=50%></center> \n",
    "<font size=2>Derived from PDSH; code in `code/make_arithmetic_transformation_plot.py`</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Suppose we have data like the above. We'll find that linear regression $y = a+bx$ doesn't model it well (left). But if we added the feature $x^2$ to give the model $y = a+bx+b_2x^2$, we could find a good fit (right)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The same idea can in principle work for $x^3$ and higher. These are called *polynomial features*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.      0.      0.   ]\n",
      " [  1.5     2.25    3.375]\n",
      " [  2.      4.      8.   ]\n",
      " [  4.     16.     64.   ]\n",
      " [  4.5    20.25   91.125]\n",
      " [  5.     25.    125.   ]\n",
      " [  6.     36.    216.   ]\n",
      " [  7.     49.    343.   ]\n",
      " [  8.     64.    512.   ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "X = np.array([[0, 1.5, 2, 4, 4.5, 5, 6, 7, 8]]).T\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X2 = poly.fit_transform(X)\n",
    "print(X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### One-hot Encoding\n",
    "\n",
    "In *one-hot encoding*, we convert a single categorical feature `f` with $n$ levels to $n$ binary features `f0`, `f1`, etc:\n",
    "\n",
    "`f` | `f0` | `f1` | `f2`\n",
    "----|------|------|-----\n",
    " `a`|  1   |   0  |  0\n",
    " `b`|  0   |   1  |  0 \n",
    " `a`|  1   |   0  |  0\n",
    " `c`|  0   |   0  |  1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Of course, Scikit-Learn provides that for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "X_train = [[\"a\"], [\"b\"], [\"a\"], [\"c\"]]\n",
    "X_train_enc = OneHotEncoder(sparse=False).fit_transform(X_train)\n",
    "X_train_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The above works if we only want to transform one dataset. \n",
    "\n",
    "But if we have a test set as well, we should use the same `OneHotEncoder` to transform it, and again we should not re-`fit` it on the test set. If the test set doesn't contain the same **set** of categories, things would break. Eg, in the following case our test set would have the wrong number of columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = [[\"c\"], [\"b\"]]\n",
    "X_test_enc = OneHotEncoder(sparse=False).fit_transform(X_test)\n",
    "X_test_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the best way to do it is to keep the `OneHotEncoder` object, `fit` on the training set and then `transform` both the training and test sets with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe = OneHotEncoder(sparse=False).fit(X_train)\n",
    "X_train_enc = ohe.transform(X_train)\n",
    "X_test_enc = ohe.transform(X_test)\n",
    "X_test_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Conclusion\n",
    "\n",
    "Scikit-Learn gives us lots of methods for feature **selection** and feature **engineering**, and we've seen a few of the most important and simplest ones.\n",
    "\n",
    "Data hygiene is important: eg when selecting features, normalising, or one-hot encoding, we use only the training set to decide **what** to do, then **do that** to both the training set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
