{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aba4d4c",
   "metadata": {},
   "source": [
    "<center><img src=img/MScAI_brand.png width=70%></center>\n",
    "\n",
    "# Scikit-Learn and OOP: Exercises and Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f428bb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import doctest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f9b9dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class C:\n",
    "    def __init__(self, data=17):\n",
    "        self.data = data\n",
    "    def __repr__(self):\n",
    "        return f\"C({self.data})\"\n",
    "    def __lt__(self, other):\n",
    "        return self.data < other.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ea1b9f",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Run this code and explain the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87665c9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<=' not supported between instances of 'C' and 'C'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_852/1867972097.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m17\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m18\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: '<=' not supported between instances of 'C' and 'C'"
     ]
    }
   ],
   "source": [
    "C(17) <= C(18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995f6f3c",
   "metadata": {},
   "source": [
    "This is a `TypeError` because `__le__` does not exist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3167ee16",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Run this code and explain the result. **Hint**: the special `id` function in Python gets the **location** of the object in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b90c6fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C(17) == C(17) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2f0082",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "```python\n",
    "C(17) == C(17)\n",
    "```\n",
    "\n",
    "is `False` because if an object doesn't have `__eq__`, Python will fall back to comparing with `id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eadcceb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2642756520256"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(C(17))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30b42e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2642756520704"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(C(17)) # a new instance, so a new location in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b9634c",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Edit the definition of `C`, implementing `__eq__` and `__le__`, to fix the problems above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6afffa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class C:\n",
    "    \"\"\"\n",
    "    >>> C(17) <= C(18)\n",
    "    True\n",
    "    >>> C(17) == C(17)\n",
    "    True\n",
    "    \"\"\"\n",
    "    def __init__(self, data=17):\n",
    "        self.data = data\n",
    "    def __repr__(self):\n",
    "        return f\"C({self.data})\"\n",
    "    def __lt__(self, other):\n",
    "        return self.data < other.data\n",
    "    def __eq__(self, other):\n",
    "        return self.data == other.data\n",
    "    def __le__(self, other):\n",
    "        return self.data <= other.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e998923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding tests in NoName\n",
      "Trying:\n",
      "    C(17) <= C(18)\n",
      "Expecting:\n",
      "    True\n",
      "ok\n",
      "Trying:\n",
      "    C(17) == C(17)\n",
      "Expecting:\n",
      "    True\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "doctest.run_docstring_examples(C, globals(), verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c977f61",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "This is a classic OOP exercise. Implement a class `Vehicle`, and then create sub-classes `Bicycle` and `Car` from it using `super`. A `Vehicle` has some number of wheels, and a colour, and a method `move`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b853fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vehicle:\n",
    "    \"\"\"\n",
    "    >>> v = Vehicle()\n",
    "    >>> v.move()\n",
    "    The vehicle is moving in an abstract kind of way\n",
    "    \"\"\"\n",
    "    def __init__(self, colour=None, n_wheels=None):\n",
    "        self.colour = colour\n",
    "        self.n_wheels = n_wheels\n",
    "    def move(self):\n",
    "        print(\"The vehicle is moving in an abstract kind of way\")\n",
    "    \n",
    "class Bicycle(Vehicle):\n",
    "    \"\"\"\n",
    "    >>> b = Bicycle(\"red\")\n",
    "    >>> b.move()\n",
    "    The red bicycle is pedalling\n",
    "    \"\"\"\n",
    "    def __init__(self, colour):\n",
    "        super().__init__(colour, 2)\n",
    "    def move(self):\n",
    "        print(f\"The {self.colour} bicycle is pedalling\")\n",
    "    \n",
    "class Car(Vehicle):\n",
    "    \"\"\"\n",
    "    >>> c = Car(\"blue\")\n",
    "    >>> c.move()\n",
    "    The blue car is combusting petrol\n",
    "    \"\"\"\n",
    "    def __init__(self, colour):\n",
    "        super().__init__(colour, 4)\n",
    "    def move(self):\n",
    "        print(f\"The {self.colour} car is combusting petrol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00adba74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding tests in NoName\n",
      "Trying:\n",
      "    v = Vehicle()\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    v.move()\n",
      "Expecting:\n",
      "    The vehicle is moving in an abstract kind of way\n",
      "ok\n",
      "Finding tests in NoName\n",
      "Trying:\n",
      "    b = Bicycle(\"red\")\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    b.move()\n",
      "Expecting:\n",
      "    The red bicycle is pedalling\n",
      "ok\n",
      "Finding tests in NoName\n",
      "Trying:\n",
      "    c = Car(\"blue\")\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    c.move()\n",
      "Expecting:\n",
      "    The blue car is combusting petrol\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "doctest.run_docstring_examples(Vehicle, globals(), verbose=True)\n",
    "doctest.run_docstring_examples(Bicycle, globals(), verbose=True)\n",
    "doctest.run_docstring_examples(Car, globals(), verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1788c62",
   "metadata": {},
   "source": [
    "\n",
    "### Exercise: predict the mode\n",
    "\n",
    "In many machine learning scenarios it's good to create a simple **baseline** to compare a more sophisticated algorithm against. In classification, one simple example is to predict the **mode** -- the most common $y$ value in the training data, ignoring the $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8dc78f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def mode(y):\n",
    "    \"\"\"\n",
    "    Example: Counter(\"aaba\") returns (item, count) tuples ordered by count:\n",
    "    [('a', 3), ('b', 1)]\n",
    "    So the most common item is at [0][0]\n",
    "    \"\"\"\n",
    "    return Counter(y).most_common()[0][0]\n",
    "mode(['a', 'a', 'b', 'a'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061c2d01",
   "metadata": {},
   "source": [
    "Create a `ModePredictor` class by refactoring the above code. Inherit from Scikit-Learn `BaseEstimator` and `ClassifierMixin`. Compare its (training) classification accuracy on the dataset below against another classifier, such as `RandomForestClassifier`. Remember, we should **inherit** classification accuracy, not implement it ourselves!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e5881e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8eff62dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.27</td>\n",
       "      <td>healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.41</td>\n",
       "      <td>healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.46</td>\n",
       "      <td>unhealthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.47</td>\n",
       "      <td>healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.64</td>\n",
       "      <td>healthy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     X0    X1          y\n",
       "0  0.66  0.27    healthy\n",
       "1  0.57  0.41    healthy\n",
       "8  0.96  0.46  unhealthy\n",
       "2  0.48  0.47    healthy\n",
       "3  0.55  0.64    healthy"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/unbalanced.csv\", index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d55c8afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"X0\", \"X1\"]].values\n",
    "y = df[\"y\"].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1105d917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.32, 0.39],\n",
       "       [0.66, 0.27],\n",
       "       [0.29, 0.14],\n",
       "       [0.97, 0.74],\n",
       "       [0.66, 0.29],\n",
       "       [0.32, 0.46],\n",
       "       [0.81, 0.72],\n",
       "       [0.22, 0.87],\n",
       "       [0.17, 0.18],\n",
       "       [0.1 , 0.76]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7ea0147b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['healthy', 'healthy', 'healthy', 'unhealthy', 'healthy', 'healthy',\n",
       "       'healthy', 'healthy', 'healthy', 'healthy'], dtype=object)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6aee4661",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModePredictor(BaseEstimator, ClassifierMixin):\n",
    "    def fit(self, X, y):\n",
    "        # Example: Counter(\"aaba\") returns (item-count) tuples ordered by count:\n",
    "        # [('a', 3), ('b', 1)]\n",
    "        # So the most common item is at [0][0]\n",
    "        self.mode = Counter(y).most_common()[0][0]\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        # ignore X, return mode!\n",
    "        # we have to return a prediction yhat for *each* element of the query X\n",
    "        return np.array([self.mode for _ in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d88020",
   "metadata": {},
   "source": [
    "When we run the code below we should see a table of results like this:\n",
    "\n",
    "```python\n",
    "ModePredictor(): 0.90\n",
    "RandomForestClassifier(): 0.90\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7c0dd887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModePredictor(): 0.90\n",
      "RandomForestClassifier(): 0.90\n"
     ]
    }
   ],
   "source": [
    "clfs = [ModePredictor(), RandomForestClassifier()]\n",
    "for clf in clfs:\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(f\"{clf}: {clf.score(X_test, y_test):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee165a7",
   "metadata": {},
   "source": [
    "We see the RF does great, 0.9 - but the mode predictor does the same, because this data is highly imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71a8e67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
