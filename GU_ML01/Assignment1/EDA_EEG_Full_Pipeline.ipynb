{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  },
  "title": "EDA_EEG_Full_Pipeline"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full EDA pipeline for EEG_Train_Binary.csv\n",
    "\n",
    "This notebook runs a ready-to-run exploratory data analysis (EDA) pipeline for the EEG CSV dataset (X1..X16 channels + binary target y). It performs basic file inspection, visualization of raw waveforms, univariate summaries, correlation/VIF checks, PCA, PSD/band-power extraction, simple feature engineering, and a baseline model evaluation.\n",
    "\n",
    "Usage:\n",
    "- Update CSV_PATH to point to the file on disk (by default it's set to the repository-relative path you provided).\n",
    "- If you know the sampling frequency (fs) of the EEG recordings, set `FS` accordingly; default is 128 Hz.\n",
    "\n",
    "Requirements: pandas, numpy, matplotlib, seaborn, scipy, scikit-learn, statsmodels, tqdm\n",
    "\n",
    "Run all cells from top to bottom."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Optional: install missing packages (uncomment if needed)\n",
    "# !pip install pandas numpy matplotlib seaborn scipy scikit-learn statsmodels tqdm\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.signal import welch\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sns.set(style='whitegrid')\n",
    "%matplotlib inline"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters — set these before running\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Path to CSV. Update if needed.\n",
    "CSV_PATH = \"GU_ML01/Assignment1/EEG_Train_Binary.csv\"\n",
    "# If you prefer pointing to a local copy, change CSV_PATH accordingly, e.g. '/mnt/data/EEG_Train_Binary.csv'\n",
    "\n",
    "# Sampling frequency (Hz) for PSD / bandpower. Change if you know the actual FS.\n",
    "FS = 128\n",
    "\n",
    "# Target column name\n",
    "TARGET_COL = 'y'\n",
    "\n",
    "# Output prefix\n",
    "OUT_PREFIX = Path(CSV_PATH).stem + \"_eda\"\n",
    "\n",
    "print('CSV_PATH =', CSV_PATH)\n",
    "print('FS =', FS)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load CSV\n",
    "p = Path(CSV_PATH)\n",
    "if not p.exists():\n",
    "    # Try to load from current working directory or the raw GitHub URL if file not present\n",
    "    print(f\"File not found at {CSV_PATH}. Please place the CSV in that path or update CSV_PATH.\")\n",
    "else:\n",
    "    df = pd.read_csv(p)\n",
    "    print('Loaded:', p, 'shape=', df.shape)\n",
    "\n",
    "try:\n",
    "    df\n",
    "except NameError:\n",
    "    raise SystemExit('Dataset not found. Set CSV_PATH to the correct file location and re-run.')\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic inspection: shape, dtypes, head/tail, missing values, duplicates, class balance, and summary stats\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print('Shape:', df.shape)\n",
    "print('\\nColumns:')\n",
    "print(df.columns.tolist())\n",
    "print('\\nDtypes:')\n",
    "print(df.dtypes)\n",
    "\n",
    "display(df.head())\n",
    "display(df.tail())\n",
    "\n",
    "print('\\nMissing values per column:')\n",
    "print(df.isna().sum())\n",
    "print('\\nMissing percent per column:')\n",
    "print((df.isna().mean()*100).round(3))\n",
    "print('\\nDuplicate rows:', df.duplicated().sum())\n",
    "\n",
    "if TARGET_COL in df.columns:\n",
    "    print('\\nClass balance for', TARGET_COL)\n",
    "    print(df[TARGET_COL].value_counts(dropna=False))\n",
    "    print((df[TARGET_COL].value_counts(normalize=True, dropna=False)*100).round(3))\n",
    "\n",
    "print('\\nSummary statistics (numeric columns):')\n",
    "display(df.select_dtypes(include=[np.number]).describe().T)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize raw samples and per-channel summary\n",
    "- Plot a few example rows (multi-channel waveform per row)\n",
    "- Plot mean ± std across dataset per channel\n",
    "- Histograms and boxplots for channels"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "channels = [c for c in numeric_cols if c != TARGET_COL]\n",
    "print('Channels detected:', channels)\n",
    "\n",
    "# Plot first N rows\n",
    "N = 6\n",
    "n = min(N, len(df))\n",
    "fig, axes = plt.subplots(n, 1, figsize=(14, 2.2*n), sharex=True)\n",
    "for i in range(n):\n",
    "    axes[i].plot(df.loc[i, channels].values, marker='o')\n",
    "    axes[i].set_ylabel(f'row {i} (y={df.loc[i, TARGET_COL] if TARGET_COL in df.columns else \"NA\"})')\n",
    "    axes[i].grid(True)\n",
    "axes[-1].set_xticks(range(len(channels)))\n",
    "axes[-1].set_xticklabels(channels, rotation=45)\n",
    "plt.suptitle('Example rows: channel values across X1..X16')\n",
    "plt.tight_layout(rect=[0,0,1,0.97])\n",
    "plt.show()\n",
    "\n",
    "# Mean ± std per channel\n",
    "mean = df[channels].mean()\n",
    "std = df[channels].std()\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.errorbar(range(len(channels)), mean, yerr=std, fmt='-o')\n",
    "plt.xticks(range(len(channels)), channels, rotation=45)\n",
    "plt.title('Mean ± std across dataset per channel')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Histograms\n",
    "df[channels].hist(bins=40, figsize=(14,10))\n",
    "plt.suptitle('Channel histograms')\n",
    "plt.show()\n",
    "\n",
    "# Boxplot across channels\n",
    "plt.figure(figsize=(14,6))\n",
    "sns.boxplot(data=df[channels], orient='h')\n",
    "plt.title('Boxplots for channels (horizontal)')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation matrix and VIF\n",
    "Compute Pearson correlations and Variance Inflation Factor (VIF) to spot multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "corr = df[channels].corr()\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(corr, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation matrix')\n",
    "plt.show()\n",
    "\n",
    "# Top correlated pairs\n",
    "pairs = []\n",
    "cols = corr.columns.tolist()\n",
    "for i in range(len(cols)):\n",
    "    for j in range(i+1, len(cols)):\n",
    "        pairs.append((cols[i], cols[j], corr.iloc[i,j]))\n",
    "pairs_sorted = sorted(pairs, key=lambda x: abs(x[2]), reverse=True)\n",
    "print('\\nTop absolute correlation pairs (top 10):')\n",
    "for a,b,v in pairs_sorted[:10]:\n",
    "    print(f\"{a} - {b}: {v:.4f}\")\n",
    "\n",
    "# VIF calculation (requires no NaNs)\n",
    "X = df[channels].fillna(0).values\n",
    "vif_data = pd.Series([variance_inflation_factor(X, i) for i in range(X.shape[1])], index=channels)\n",
    "print('\\nVIF per channel:')\n",
    "print(vif_data.sort_values(ascending=False))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA and dimensionality reduction\n",
    "Standardize, run PCA, plot explained variance and PCA(2) colored by class."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "scaler = StandardScaler()\n",
    "Xs = scaler.fit_transform(df[channels].fillna(0).values)\n",
    "pca = PCA()\n",
    "pca.fit(Xs)\n",
    "cumvar = np.cumsum(pca.explained_variance_ratio_)\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(np.arange(1, len(cumvar)+1), cumvar*100, '-o')\n",
    "plt.xlabel('n components')\n",
    "plt.ylabel('Cumulative explained variance (%)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "X_pca2 = PCA(n_components=2).fit_transform(Xs)\n",
    "plt.figure(figsize=(6,6))\n",
    "if TARGET_COL in df.columns:\n",
    "    sns.scatterplot(x=X_pca2[:,0], y=X_pca2[:,1], hue=df[TARGET_COL].astype(str), alpha=0.7)\n",
    "else:\n",
    "    plt.scatter(X_pca2[:,0], X_pca2[:,1], alpha=0.7)\n",
    "plt.title('PCA(2)')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSD and bandpower feature extraction\n",
    "We'll compute Welch PSD for channels and extract bandpower in canonical EEG bands.\n",
    "Note: FS default is 128 Hz; set FS to the correct sampling rate if known."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def bandpower(x, sf, band, nperseg=None):\n",
    "    \"\"\"Compute band power by integrating the PSD over a frequency band.\"\"\"\n",
    "    if nperseg is None:\n",
    "        nperseg = min(len(x), max(256, int(sf*2)))\n",
    "    f, Pxx = welch(x, fs=sf, nperseg=nperseg)\n",
    "    f = np.asarray(f)\n",
    "    Pxx = np.asarray(Pxx)\n",
    "    idx = np.logical_and(f >= band[0], f <= band[1])\n",
    "    if not np.any(idx):\n",
    "        return 0.0\n",
    "    return np.trapz(Pxx[idx], f[idx])\n",
    "\n",
    "bands = {\n",
    "    'delta': (0.5, 4),\n",
    "    'theta': (4, 8),\n",
    "    'alpha': (8, 13),\n",
    "    'beta': (13, 30),\n",
    "    'gamma': (30, 45)\n",
    "}\n",
    "\n",
    "# Example: plot PSD for first channel\n",
    "ch0 = channels[0]\n",
    "f, Pxx = welch(df[ch0].values, fs=FS, nperseg=min(256, len(df[ch0].values)))\n",
    "plt.semilogy(f, Pxx)\n",
    "plt.xlim(0, 60)\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('PSD')\n",
    "plt.title(f'Welch PSD - {ch0}')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Compute bandpower per channel for the whole row (i.e., treating each row as a short timeseries of 16 samples ONLY if rows are epochs)\n",
    "# NOTE: This dataset appears to have each row represent values across 16 channel *samples*, not a long timeseries per channel.\n",
    "# If each row is an epoch with 16 timepoints per channel, bandpower on 16 samples is unreliable. However, we still compute PSD/bandpower\n",
    "# for analysis — if your dataset actually places time along columns and channels as rows, you should transpose accordingly.\n",
    "\n",
    "bandpower_df = pd.DataFrame(index=df.index)\n",
    "for bname, bband in bands.items():\n",
    "    vals = []\n",
    "    for idx in range(len(df)):\n",
    "        # assemble a pseudo-timeseries by taking values across channels for this row\n",
    "        # This is dataset-specific: here we compute bandpower across the 16-channel vector\n",
    "        x = df.loc[idx, channels].values.astype(float)\n",
    "        try:\n",
    "            bp = bandpower(x, sf=FS, band=bband, nperseg=min(64, len(x)))\n",
    "        except Exception:\n",
    "            bp = np.nan\n",
    "        vals.append(bp)\n",
    "    bandpower_df[f'bp_{bname}'] = vals\n",
    "\n",
    "display(bandpower_df.head())\n",
    "print('\\nBandpower summary:')\n",
    "display(bandpower_df.describe().T)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering: per-channel simple stats and aggregation\n",
    "We'll compute mean, std, skew, kurtosis, RMS, energy (sum squares), and zero-crossing count per row across channels, then combine with bandpower features to form a feature table for baseline modelling."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "features = pd.DataFrame(index=df.index)\n",
    "\n",
    "# Per-row aggregate stats across channels\n",
    "features['ch_mean'] = df[channels].mean(axis=1)\n",
    "features['ch_std'] = df[channels].std(axis=1)\n",
    "features['ch_median'] = df[channels].median(axis=1)\n",
    "features['ch_min'] = df[channels].min(axis=1)\n",
    "features['ch_max'] = df[channels].max(axis=1)\n",
    "features['ch_skew'] = df[channels].apply(lambda row: skew(row), axis=1)\n",
    "features['ch_kurtosis'] = df[channels].apply(lambda row: kurtosis(row), axis=1)\n",
    "features['ch_rms'] = np.sqrt((df[channels]**2).mean(axis=1))\n",
    "features['ch_energy'] = (df[channels]**2).sum(axis=1)\n",
    "features['ch_zc'] = df[channels].apply(lambda row: ((np.diff(np.sign(row.values))!=0).sum()), axis=1)\n",
    "\n",
    "# Add bandpower features computed earlier\n",
    "features = pd.concat([features, bandpower_df.add_prefix('row_')], axis=1)\n",
    "\n",
    "print('Feature matrix shape:', features.shape)\n",
    "display(features.head())\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline modeling: logistic regression with StratifiedKFold\n",
    "We will use ch_mean, ch_std, ch_rms, energy, zc and bandpower features as a simple baseline.\n",
    "All preprocessing (scaling) is done inside CV to avoid leakage."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "if TARGET_COL not in df.columns:\n",
    "    print('Target column not found in dataset; skipping baseline modelling.')\n",
    "else:\n",
    "    X = features.copy()\n",
    "    y = df[TARGET_COL].values\n",
    "\n",
    "    clf = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('lr', LogisticRegression(max_iter=1000))\n",
    "    ])\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    acc_scores = []\n",
    "    auc_scores = []\n",
    "    for train_idx, test_idx in skf.split(X, y):\n",
    "        Xtr, Xte = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        ytr, yte = y[train_idx], y[test_idx]\n",
    "        clf.fit(Xtr, ytr)\n",
    "        yp = clf.predict(Xte)\n",
    "        yp_prob = clf.predict_proba(Xte)[:,1] if hasattr(clf.named_steps['lr'], 'predict_proba') else None\n",
    "        acc_scores.append(accuracy_score(yte, yp))\n",
    "        if yp_prob is not None:\n",
    "            auc_scores.append(roc_auc_score(yte, yp_prob))\n",
    "    print('Baseline LogisticRegression on aggregated features')\n",
    "    print('Accuracy (5-fold):', np.round(np.mean(acc_scores),4), '±', np.round(np.std(acc_scores),4))\n",
    "    if auc_scores:\n",
    "        print('ROC AUC (5-fold):', np.round(np.mean(auc_scores),4), '±', np.round(np.std(auc_scores),4))\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save features and summary\n",
    "We save the engineered features and a JSON summary of basic inspection."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "features_out = OUT_PREFIX + '_features.csv'\n",
    "summary_out = OUT_PREFIX + '_basic_summary.json'\n",
    "\n",
    "features.to_csv(features_out, index=True)\n",
    "print('Saved features to', features_out)\n",
    "\n",
    "basic_summary = {\n",
    "    'shape': df.shape,\n",
    "    'columns': df.columns.tolist(),\n",
    "    'dtypes': df.dtypes.apply(lambda x: str(x)).to_dict(),\n",
    "    'missing': df.isna().sum().to_dict(),\n",
    "    'duplicate_rows': int(df.duplicated().sum()),\n",
    "    'class_balance': df[TARGET_COL].value_counts(dropna=False).to_dict() if TARGET_COL in df.columns else None\n",
    "}\n",
    "with open(summary_out, 'w') as f:\n",
    "    json.dump(basic_summary, f, indent=2)\n",
    "print('Saved summary to', summary_out)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short narrative\n",
    "\n",
    "This notebook performed a complete EDA pass: file load and basic checks, visual inspection of example rows, distribution and outlier views, correlation and VIF checks, PCA, a (conservative) bandpower computation, per-row aggregation feature engineering, and a baseline logistic regression evaluated with StratifiedKFold. I saved the engineered features and a basic summary JSON. Adjust FS, CSV_PATH, and how bandpower is computed depending on the true data layout (if columns are time samples vs channels).\n",
    "\n",
    "Next you can:\n",
    "- If you know the actual sampling frequency and the correct orientation (rows=epochs, columns=timepoints), recompute PSD/bandpower using per-channel time series instead of the current per-row approach.\n",
    "- Add richer signal features (Hjorth, entropy, wavelet bands) and try tree-based models or CNNs on raw epoch arrays.\n",
    "- If you have subject/session metadata, implement GroupKFold to avoid leakage.\n",
    "\n",
    "If you'd like, I can now:\n",
    "- Convert this notebook into a polished Jupyter Notebook file for direct download, or\n",
    "- Modify the bandpower extraction to treat columns as time samples per channel if you confirm that's the case.\n"
   ]
  }
 ]
}